**Step 2: Supervised Fine-Tuning (SFT) for Enhanced Readability and Performance**

_Example for Step 2_

> Now that we have our puppy and we are not going to ignore his training, we need to establish a couple things. One is a reward marker, usually a clicker, and some form of negative marker, generally a "No", "Opps", a tug on the leash, or some other sort of sound to let the dog know the exact location they did well, or where they went wrong.

**Step 2**

To **improve the clarity, consistency, and reasoning accuracy** of the final model, DeepSeek applies a **targeted supervised fine-tuning (SFT) phase** before reinforcement learning. This step refines the base model using a curated dataset consisting of **a few thousand high-quality completions** generated by R1-Zero.

**Key Refinement Techniques**

While this fine-tuning process is relatively lightweight, it plays a critical role in improving output quality. DeepSeek employs several optimization techniques, including:

*   **Few-shot prompting** with structured, multi-step reasoning examples to guide model responses.
*   **Direct prompting for detailed answers**, encouraging self-reflection and verification within responses.
*   **Formatting model outputs for readability**, ensuring consistency in structure and presentation.
*   **Post-processing by human annotators**, refining data quality and improving response reliability.

For teams looking to replicate this process, **any of these techniques can be applied**, and leveraging DeepSeek R1 itself may provide the most efficient path forward.

This supervised fine-tuning step **optimizes the modelâ€™s training trajectory**, making it easier to develop **self-correction and reasoning behaviors** during reinforcement learning. It enhances the likelihood that the model will exhibit advanced cognitive traits such as:

*   **Self-verification:** "Let me double-check my work."
*   **Error recognition:** "That answer seems incorrect; let me refine it."

By laying this groundwork early, the reinforcement learning phase can **more effectively strengthen these reasoning capabilities**, resulting in a more reliable and interpretable AI model.