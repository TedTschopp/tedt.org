1985 - April - BYTE Artificial_Intelligence: CONNECTIONS

**BY JEROME A. FELDMAN**

**Massive parallelism in natural and artificial intelligence**

A FUNDAMENTAL PREMISE of artificial intelligence (AI) is that intelligent behavior can be simulated on digital computers. Great progress has been made in AI, with far-reaching practical and scientific consequences. Despite successes with quite specific problems, however, many researchers feel that the general questions of intelligence have barely been touched. The field of AI is nowhere near possessing the ability to simulate the natural intelligence of a small child or even a simple animal. Yet the basic computing speed of modern computers is about a million times faster than the firing rate of a neuron, which is in the millisecond range.

The situation becomes even more striking when we consider the speed of human responses on simple tasks. When asked to carry out a wide range of tasks, such as naming a picture or deciding if some sound is an English noun, people can respond correctly in about half a second. This means that the brain, a device composed of neural elements with a basic computing speed of a few milliseconds, can solve difficult problems of vision and language in a few hundred milliseconds (0.5 second - 500 milliseconds), or about 100 steps. The best AI programs for these tasks are not nearly as general and require millions of computational time steps. Of course, the brain is a parallel device, while essentially all of our existing machines are sequential, executing one instruction at a time. But what are the algorithms used in this massively parallel natural computer?

The exploration of possible algorithms for this radically different architecture forms the cornerstone of a promising new branch of artificial intelligence. Workers in the field use an abstract computer that is as similar as possible to the brain in structure and performance. This approach permits AI researchers to collaborate closely with colleagues in the brain and behavioral sciences: a good algorithm for some task on a computational model can also serve as a prototype theory for the mechanisms the brain uses for the same problem. Some psychologists have found that constructing massively parallel computational models to fit their data is far easier than constructing models based on sequential machines. The cycle is complete when AI researchers use experimental results to guide the construction of parallel algorithms for various problems.

In addition to the 100-step restriction, using the brain as a model has other computational consequences. For one thing, the timing constraints limit a few bits of the information that can be sent from one neuron to another. This means that units cannot pass the large symbolic structures common in AI and that the computational richness must lie in the connections among units; this has led to the use of the term "connectionist" to characterize these models.

Using the brain as a model does allow for quite a lot of connections, up to about 10,000 per unit. The total number of neurons (about 100 billion) seems large but is actually a major limitation. Since vision has a million parallel inputs, any algorithm requiring n² units would not fit. Also, the brain grows no new units and essentially no new connections, placing severe constraints on models of learning. Even with all these constraints, connectionist models are increasingly popular in AI research.

**A VISUAL MODEL**

A simple example can capture some of the flavor of connectionist computation. The cube shown in figure 1 is a famous optical illusion attributed to the Swiss naturalist L. A. Necker (1832). Most people initially see the cube with the corner G closer to them, but you can also see it as a cube with vertex A closest to you. If you focus on vertex A and imagine it coming out of the paper toward you, the picture will flip to the A-closer cube. Notice that the flip takes less than a second. The Necker cube is interesting to psychologists because it will flip spontaneously between the two views if you keep looking at it. It is interesting to artificial-intelligence researchers because of what it tells us about parallel computation.

You have observed how quickly the Necker cube flips, and you know how slow the underlying human computing elements are. A sequential program running on such a slow device could probably not perform this task. But the situation is much more complex. Both human and computer vision require several levels of processing (see "Vision" by Dana H. Ballard and Christopher M. Brown on page 245). Typical levels include edge segments, lines, vertices, and object descriptions. The edges and lines are the same for both the A-closer and G-closer cubes, but many other visual features are seen differently in the two views. A few of these are given in figure 2. Remarkably, our visual system simultaneously flips all these perceptual decisions from one mutually consistent reading of the cube to the other. This illustrates the key cooperative property of massively parallel computation and why it is conceptually different from von Neumann computation on standard machines.

The Necker cube also illustrates some of the details of the connectionist paradigm, as shown in figure 2. In our models each item of interest is represented as a computational unit, with connections to many other units. Each unit has a level of activity (say between -10 and +10) and automatically sends the value of this activity along all its outgoing connections. In the network of figure 2, units that are mutually consistent (for example, "H closer than G" and "G hidden") have connections to each other. Units that are mutually exclusive, such as "G hidden" and "G showing" are connected with circle-tipped links denoting negative connections. The only other information needed for a complete model is the rule by which a unit computes its new activity from its inputs and its old activity. We can assume that the units compute the average of their positive and negative inputs. Networks like figure 2 are not very sensitive to the exact choice of unit computation rules; this is one of the reasons for their attractiveness. Units that are all mutually connected by negative links are said to comprise a "winner-take-all" network. Such networks are one of the main decision mechanisms in connectionist models and have known neurophysiological analogs.

Much of the effort in massively parallel AI is dedicated to using computational frameworks like that in figure 2 to build models of intelligent activity. Advantages of this approach include its link to natural intelligence, increased noise resistance, and ease of implementation on parallel hardware. But the main advantage of the connectionist approach is that it provides a much better way of specifying some computations. I know of no other way to describe the Necker-cube phenomenon that is as clear and concise as the model in figure 2.

**A NATURAL-LANGUAGE APPLICATION**

Researchers in AI and related areas of cognitive science are using connectionist models to study a variety of tasks. Vision is an area where massive parallelism fits naturally. Ballard and Brown's article describes some of the success in that area. The methodology is also effective in natural-language research. Although it is a less obvious application, researchers have had some very nice results in that domain.


One problem that has worked out particularly well is disambiguation. Consider what happens when you hear or read a sentence such as: "Bob threw a ball." You automatically assign a meaning to each word, most likely interpreting "threw" as "propelled" and "ball" as "a sphere," despite the fact that most words in English are ambiguous. For example, the words "threw" and "ball" both suggest quite different meanings in the sentence "Bob threw a ball for charity." The problem is to develop algorithms that exhibit this behavior and help explain its basis. Linguists and other cognitive scientists have worked at length on these issues and have developed sophisticated theories. The contribution of AI has been to encode these theories in programs so that they can be tested and, if correct, used in applications. As with the Necker-cube example, massively parallel models appear to constitute the best way to carry out the encoding.

In language, as in vision, the theory calls for several distinct levels of representation and processing. Figure 3 shows three of these levels. The lexical level comes after the individual sounds or letters have been formed into words; this level corresponds to what you would look up in a dictionary. The word-sense level corresponds to the various meanings listed under a word in the dictionary—the problem, of course, is to pick the right one. The case level comes from linguistic theory and conveys the idea of the different roles that words can play in a sentence. The cases used here are:

- agent: the person or thing carrying out an action
- object: the thing acted upon
- recipient: beneficiary of the action

The key linguistic insight is that there are constraints on the possible word senses that can fill various case roles. For example, you cannot throw (propel) a ball (dance), so these are incompatible. Nor can you throw (sponsor) a ball (sphere). The only other fact used in the model is that some word senses are more frequent than others.

We can now see how figure 3 presents a connectionist computational model of disambiguation. As with the Necker cube, compatible units have positive links and incompatible units have negative, winner-take-all, links. The model assumes that as each word reaches the lexical level it spreads activity to the various word senses to which it is connected. Since the more frequent sense has a greater weight, it will tend to dominate less frequent senses. As additional words come in, they will activate more word senses and case roles. The simple sentence "Bob threw a ball" will activate a mutually consistent set of units, and we will never notice the alternatives.  The additional words "for charity" will activate the "dance" meaning of ball. This will weaken the "sphere" meaning, which will in turn reduce the activity of "propel" because the sentence no longer has a suitable object. Thus, an alternative stable coalition develops and suppresses the original interpretation. The two alternative coalitions are quite similar to the two readings of the Necker cube. This is no accident—the idea of a cooperative-competitive network pervades parallel models. Work on language problems such as disambiguation is quite advanced and offers simple explanations of many phenomena. For example, a context that biased us toward the "dance" sense of ball would be modeled as providing that meaning with a head start in its competition with "sphere." Again, the massively parallel paradigm is the simplest way to express this idea.

**KNOWLEDGE REPRESENTATION**

The Necker-cube and disambiguation examples are both instances of what AI researchers call recognition problems. Several other problems are like this, but many are not. Can we apply massively parallel models to other traditional AI issues such as knowledge representation and inference? Researchers have completed much less work along these lines but have made some promising starts. The example in figure 4 should convey the flavor of this work.

The standard way to explore the issue of knowledge representation and inference is in terms of programs that can answer questions. AI approaches to the development of question-answering systems are numerous (Roger Schank and Larry Hunter present one in "The Quest to Understand Thinking" on page 143), but the approaches all have the same basic requirements: You need a way to store the knowledge, to pose questions, and to compute and register the answers. In a connectionist model, all of these aspects must be expressed in terms of activity spreading among simple units like those in the previous examples.

It is easiest to start with the recording of answers. In figure 4, the possible tastes of foods form a winner-take-all network, where each unit inhibits the others so only one answer will be active. The answer network is assumed to be part of a routine that also poses the questions and acts upon the answer. The units that make up the routine are assumed to be activated in sequence from left to right just like a standard program. The routine sends a question to the knowledge network by activating the appropriate units; figure 4 shows this as links, for example, from the hexagonal node to the nodes for "has-taste" and "ham." The key to the operation of this network is the operation of the triangular-shaped nodes, such as "b1." We define such a triangular unit to become active when two of its inputs are simultaneously active. In this case "ham" and "has-taste" are both on, so "b1" becomes active and activates "salty." Now the "salty" node in the knowledge network spreads activation to suppress "r-salty" back in the routine and the question is answered. The same network can answer questions such as "Name a salty meat" when activated appropriately. The answers returned by such a network will depend on context, just as people's answers do; contextual bias is again modeled by activation.

The examples of cube perception, word disambiguation, and question answering are typical of the current applications of massively parallel computational models in AI. The examples are condensed and they omit many important considerations, but they are representative of the current level of attainment. In no case are the connectionist programs as sophisticated as conventional AI systems for similar tasks. The general notion of massively parallel models in AI is quite new, and we do not yet understand where its ultimate strengths and weaknesses will lie. We do know that the development of highly parallel computers will have a marked effect on the practicality of connectionist approaches to AI.

Even if we had discovered parallel algorithms for vision and language tasks and found them efficient on parallel hardware, we would be missing a crucial element of intelligence—learning. No system that cannot incorporate new knowledge and change its behavior could be called intelligent. This is an important issue...

for all of AI but is especially critical for connectionist models. If we model our system as a set of rules, we’ll have no conceptual difficulty in adding more rules, although the question of which rules to add remains unsolved. For connectionist models, the intelligence is in the connections—but we know that the brain grows essentially no new connections. How could such a system, even in principle, incorporate new knowledge?

The neural substrate of memory and learning is one of the great unsolved scientific questions for which we certainly have no definitive answers. But connectionist theories of learning are compatible with current brain research and are computationally feasible. The key idea is that while new connections are rare, weight changes in connections appear to be common. We also know that each unit can have thousands of incoming and outgoing connections. Our hypothesis is that most of these connections are only potentially important and that learning involves strengthening the appropriate connections. Suppose, for example, the network of figure 4 needed to learn that spinach was a salty vegetable. Our model suggests that there are uncommitted triangular nodes that are weakly connected to many combinations of objects, properties, and values. In an ideal case, one of them will be linked to "spinach," "has-taste," and "salty," among other things. This unit will become highly activated by the simultaneous activation of three of its neighbors and, by strengthening its active connections, can become dedicated to the new association. This example omits many important issues: the whole learning theory is in a very primitive stage. Geoffrey Hinton, in his article "Learning in Parallel Networks" (page 265), describes a related but different learning scheme based on the assumption that concepts are represented diffusely as the activity of many, many units. All of the connectionist AI efforts badly need more theoretical and experimental work, which leads us to an active interest in parallel computers.

**PARALLEL COMPUTERS**

Clearly, massively parallel AI models have a natural fit to parallel computers. The speed of our current simulations on sequential machines is slow enough to be a major bottleneck. While some kinds of programs are hard to transfer to parallel computers, connectionist simulations are straightforward to move. If we have 10,000 processors and a network of 20,000 units to simulate, we just assign 200 units to each processor. There are technical questions about the best way to partition the network, but the basic idea is simple. It is less clear how to monitor and modify the behavior of such systems, and this leads to basic research questions in parallel computation. A number of machines with hundreds of processors are being built in various labs. Our group at the University of Rochester is working with the Butterfly multiprocessor of Bolt Beranek and Newman; we expect to have a 128-processor system running this summer. A parallel version of our connectionist simulator is one of the major driving applications for the Butterfly.

Possibly the best way to program many problems for a multiprocessor is to express them as a massively parallel network. While breaking a problem into 256 equal pieces is difficult, it may be fairly easy to express the problem’s natural parallelism and let a compiler map the result onto whatever hardware is available. This seems to be the experience of the Cal Tech group on the physics problems they have attacked, and it appears to be true for many AI problems. The success of this approach could be a coincidence, but it might be related to the fact that intelligence evolved to massively parallel hardware.

Jerome A. Feldman (University of Rochester, Rochester, NY 14627) is a professor of computer science and holds a doctorate from Carnegie-Mellon University. His research interests include programming languages and systems, AI, and the problems of parallel algorithms and biological computation.