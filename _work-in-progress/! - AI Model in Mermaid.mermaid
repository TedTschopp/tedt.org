%%{ init: { "flowchart": { "defaultRenderer": "elk" } } }%%

%% Mermaid.js diagram of a full LLM architecture with context-aware orchestration
graph LR

subgraph "<i class='fas fa-user'></i> - Users"
    U1["<i class='fas fa-user'></i> - AI Application User"]
    U2[AI Goverance User]
    U3[Model Builder]
end

%% ========= Safety & Governance =============
subgraph "Safety & Governance"
    E1[Guardrails]
    E2[Telemetry]
    E3[Bias Detection]
    E4[Access Control]
end

%% ========= Application Layer =============
%% - Application
subgraph "Application A"
    D1[Session Memory]
    C1[System Prompt Library]
    F1["<i class='fas fa-user'></i> - User Input"]
    F2["Traditional Business Logic"]
    OUT["<i class='fas fa-user'></i> - Final Response to User"]
end

%% - Application 
subgraph "Application B"
    D3[External API]
end

%% -- Agent Layer --
subgraph "AI Agent"
    D5[Function Call Suggestion] 
    %% D5 ← output generated by model
    D2a[Function Parser]
    D2b[Tool Executor]
    D4["Tool Output Injected as Context"]
end

%% -- Prompting / Context Injection --
subgraph "Prompt Engineering & Orchestration"
    C2[User Prompt]
    C5[System Prompt]
    C3["Agent Response"]
    C4[Finalized Prompt]

end


%% -- Core Model --
subgraph "Core Model"
    A1[Tokenizer]
    A2[Embedding Layer]
    A3["Decoder Stack (LLM)"]
    A4[Base Model Weights]
end

%% -- Adaptation & Tuning --
subgraph "Model Training"
    direction RL
    B1[1 Pretraining - Initial training of the model on large-scale, general-purpose datasets]
    B3[2 Supervised Fine-Tuning - Model is trained on specific task-labeled datasets]
    B2[3 Instruction Tuning - Specialized form of supervised fine-tuning where the model is trained to follow natural language instructions.]
    B4[4 Reinforcement Learning from Human Feedback - Refines model behavior using a reward model trained on human preferences]
    B5[5 Adapters: LoRA - Low-Rank Adaptation, Quantized LoRA - Applied post-core-training for domain- or use-case-specific customization.]
    B1:::ModelBuilder --> B3:::ModelBuilder --> B2:::ModelBuilder --> B4:::ModelBuilder --> B5
end

%% ========= Flow Arrows ============

C1 --"Select Proper Prompt from Library"--> C5

C5 --> C4


%% Prompt construction and entry into model

E1 e35@--> B4

%% User Input to User Prompt
F1 e2@--"User Input to User Prompt" --> C2

%% User Input to Traditional app
F1 e3@--"User Input to Traditiona App" --> F2

F2 e4@--> OUT

%% User prompt added
C2 e5@--> C4 

%% Context (docs/tool output) added
C3 e6@--> C4 

%% Session memory merged into prompt
D1 e7@--> C4 

%% Final prompt is tokenized
C4 e8@--"Final Prompt is Tokenized" --> A1

%% Core model stack

%% Embeddings, decoder, weights
A1 e9@-->  A2 e10@--> A3  

A4 e11@--"Weights Loaded Into"--> A3


%% Model generates a function call suggestion
A3 e12@--> D5 

%% Model generates final response and passes control back to the application
A3 e13@--> F2 

%% Function calling orchestration
D5 e14@--> D2a e15@--"Tool Selection"--> D2b e16@--"External Tool Needed"--> D3 e17@--"API Response"--> D4 e18@--> C3

D2b --> D4

%% Training flow

B5 e22@--> A4
B5 e23@--> A3

%% Governance as monitoring / constraint overlays (dashed lines)

%% Guardrails monitor / filter outputs
E1 e24@--"Guardrails monitor / filter outputs"--> A3 

%% Telemetry listens for model metrics
E2 e25@--"Telemetry listens for model metrics"--> A3 

%% Bias detection applied post-generation
E3 e26@--"Bias Detection Applied Post Generation"--> A3 

%% Access control wraps orchestration layer
E4 e27@--"Access contols Wraps Orcastration layer"--> D2a 


%% User Authenticates
U1 e281@--"User Authenticates"--> E4 

%% User Uses App
U1 e29@--"User Input" --> F1
OUT e30@--"Output to User" --> U1

%% AI Governace 
U2 e31@--"Validate and Enhance"-->E1
U2 e32@--" Observe Stats"--> E2
U2 e33@--"Observe Bias"--> E3

F2 e34@--"Business Problem Context"--> D1

U3 ModelBuilder@--"Build Model"--> B1

U3 BuildGuardrails@--"Establish Guardrails for Build"--> E1



%% ========= Inline Labels as Comments ============
%% D5 = Function Call JSON blob emitted by model (e.g., OpenAI format)
%% D2a = Function parser (schema validator, type checker)
%% D2b = Tool executor (API caller, LangChain tool runner)
%% D4 = Tool output becomes part of next-turn prompt context
%% OUT = Final model response after reasoning / tool feedback

  classDef animate stroke-dasharray: 9,5,stroke-dashoffset: 900,animation: dash 25s linear infinite;
  class e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12,e13,e14,e15,e16,e17,e18,e23,e24,e25,e26,e27,e28,e29,e30,e31,e32,e33,e34 animate

  classDef ModelBuilder stroke: Red; 
  class e19,e20,e21,e22,e23,e35,ModelBuilder,BuildGuardrails ModelBuilder;

%% Color styles
classDef AIUser stroke:#3399FF,stroke-width:2px;
class e27 AIUser;

classDef TradApp stroke:#5C6BC0,stroke-width:2px;

classDef AIAgent stroke:#FBC02D,stroke-width:2px;

classDef Governance stroke:#26A69A,stroke-width:2px;
class e24,e25,e26 Governance;





