---
layout: post
title: 'AI in 2024: A Year in Review'
subtitle: Faster, Cheaper, More Accessible - But What's Next?
quote: The rapid democratization of AI is reshaping the industry - but with great
  power comes great responsibility.
excerpt: AI in 2024 became faster, cheaper, and more accessible—bringing powerful
  tools to more people than ever. But uneven adoption, quality issues, and rising
  environmental costs remain. For SCE, the focus in 2025 must be on closing usage
  gaps, improving data quality, and turning AI potential into measurable value.
source: Original Content
source-url: ''
call-to-action: Share your thoughts on how AI advancements in 2024 have impacted your
  work and organization
date: 2025-01-02
update: null
author:
  avatar: https://secure.gravatar.com/avatar/a76b4d6291cecb3a738896a971bfb903?s=512&d=mp&r=g
  name: Ted Tschopp
  url: https://tedt.org/
bullets:
- 18 organizations achieved GPT-4-class model breakthroughs, signaling a new era of
  innovation
- LLM prices dropped dramatically—up to 27 times cheaper than previous years
- GPT-4-class models can now run on standard laptops, reducing infrastructure dependency
- Multimodal AI became mainstream, enabling interaction across text, images, audio,
  and video
- Context lengths expanded from 4K to 2M tokens, allowing vastly more complex inputs
description: A comprehensive review of AI advancements in 2024, including breakthrough
  models, dramatic cost reductions, accessibility improvements, and multimodal capabilities
  that are reshaping the industry landscape.
seo-description: 'Discover the major AI breakthroughs of 2024: GPT-4-class models,
  27x price drops, laptop-runnable AI, multimodal capabilities, and what it means
  for 2025.'
categories:
- Artificial Intelligence
- Science
tags:
- AI review
- technology trends
- GPT-4 class models
- multimodal AI
- AI accessibility
keywords: AI 2024 review, GPT-4 class models, LLM price drops, multimodal AI, AI democratization,
  machine learning breakthroughs, artificial intelligence trends
location:
  name: Bradbury, CA
coordinates:
  latitude: 34.147
  longitude: -117.9709
image: img/AI/What-AI-in-2024-Means-for-Us-in-2025.webp
image-alt: Visual representation of AI breakthroughs in 2024
image-artist: Ted Tschopp
image-artist-URL: https://tedt.org/
image-credits: Ted Tschopp
image-credits-URL: https://tedt.org/
image-credits-artist: Ted Tschopp
image-credits-artist-URL: https://tedt.org/
image-credits-title: ''
image-description: ''
image-title: ''
mastodon-post-id: '115017674210477849'
image_width: 1456
image_height: 816
---
## Executive Summary:

### Big Picture

18 organizations achieved GPT-4-class model breakthroughs, signaling a new era of innovation.
LLM prices dropped dramatically—up to 27 times cheaper than previous years.
GPT-4-class models can now run on standard laptops, reducing reliance on expensive infrastructure.
Multimodal AI is now mainstream, enabling interaction across text, images, audio, and video.

## Key Developments

### Technical Breakthroughs:

Context lengths expanded from 4K to 2M tokens, allowing vastly more complex inputs and outputs.
Real-time voice and camera interactions revolutionized user engagement and accessibility.
Inference-scaling models enhanced reasoning capabilities for complex tasks.
Prompt-driven application generation became a widely adopted standard, accelerating innovation.

### Accessibility Revolution:

Open-source models now match proprietary systems in performance, leveling the playing field.
Apple's MLX library enabled efficient AI computing on personal devices, pushing AI adoption forward.
Synthetic training data proved effective, reducing dependency on costly real-world datasets.
Advances in energy efficiency addressed sustainability concerns while maintaining high performance.

### Why It Matters

The rapid democratization of AI is reshaping the industry. Open-source achievements like DeepSeek v3, trained for under $6M yet rivaling Claude 3.5, demonstrate how world-class AI capabilities are becoming accessible to a broader range of organizations. This shift opens up possibilities for smaller companies and non-traditional players to innovate at scale.

### My Experience

Incorporating these advancements into daily workflows has been transformative. GitHub Copilot can now integrate a multitide of models and its detailed coding assistance have streamlined development processes, while GPT-4’s voice mode has added a practical edge for learning and personal development.

### What’s Next

Looking ahead to 2025, the AI landscape is poised for further breakthroughs. With training costs plummeting and open-source performance reaching new heights, we are entering an era where powerful AI tools will be within reach for everyone—not just major tech players. This shift will drive innovation and unlock untapped potential across industries.

### Detailed Advances

1. The "GPT-4" class models are now common. - Large Language Models (LLMs) of GPT-4 caliber are now the industry standard, no longer a premium technology. Their widespread availability has democratized advanced AI capabilities across sectors. This shift has fueled innovation and competition, making cutting-edge AI tools a standard business resource.
2. Some GPT-4 class models can now run on laptops we deploy. - The development of smaller, more efficient models capable of running on personal laptops marks a turning point. This shift enables organizations to deploy powerful AI without the need for high-end infrastructure. The ability to use robust LLMs locally reduces dependency on cloud platforms and opens up new possibilities for offline use.
3. This drove LLM token prices to crash due to efficiency. - Efficiency gains in model architecture and training methodologies have driven the cost of processing tokens to all-time lows. This has reduced the financial barriers to integrating LLMs into business workflows. Lower token prices enhance the ROI for companies leveraging AI in operations, customer service, and content creation.
4. Increased competition is also dropping prices. - The proliferation of AI vendors and models has intensified market competition, further driving down costs. Businesses now have access to a broader array of AI solutions, empowering them to choose options that best align with their budgets and needs. This trend fosters innovation as companies vie to offer better, more affordable tools.
5. Multimodal vision is common with publicly available data. - Multimodal AI, particularly vision models, has become a standard feature, leveraging widely available datasets. These capabilities allow businesses to analyze images and videos with unprecedented precision. From manufacturing to retail, multimodal vision is enhancing operational efficiency and customer experiences.
6. Multimodal audio and video are starting to emerge, bringing science fiction-like changes with voice + live video. - Emerging multimodal models that combine audio, video, and text are unlocking futuristic applications. Real-time voice and video AI interactions are now possible, revolutionizing industries like telecommunication and remote collaboration. This leap brings science fiction closer to reality, reshaping how we engage with technology.
7. Prompt-driven app generation, which was in the lab late last year, is already a commodity. - What started as an experimental feature has become a mainstream tool: generating applications via prompts. This capability accelerates development timelines and lowers entry barriers for creating AI-driven software. The result is a surge in innovation and a new wave of tailored business applications.
8. Universal access to GPT-4 class models was free when launched. - For a brief period, top-tier LLMs were freely available, sparking widespread experimentation and adoption. This open access allowed companies to explore use cases without financial constraints. However, this phase was short-lived as concerns over misuse and sustainability prompted restrictions.
9. The latest models, o1, o1 Pro, o3, and o3 Pro are not free and very expensive. - The newest LLM iterations come with steep price tags, limiting access for smaller enterprises. While these advanced models offer unparalleled capabilities, their cost has raised concerns about equitable AI adoption. Businesses must carefully evaluate ROI before investing in these premium solutions.
10. “Agents” still haven’t really happened yet. - Despite hype, autonomous AI agents have yet to achieve practical utility. Current limitations in reasoning, decision-making, and reliability hinder their adoption. Businesses remain cautious, opting instead for more controlled and predictable AI tools.
11. Evals really matter. Test-driven development for LLMs is the key forward. - Evaluation frameworks have emerged as a cornerstone of effective AI implementation. Rigorous testing ensures that LLMs meet performance benchmarks and align with organizational goals. This test-driven approach reduces risks and enhances trust in AI deployments.
12. Apple Intelligence needs work. - Apple’s AI efforts in 2024 failed to meet expectations, particularly in comparison to competitors. Gaps in functionality and performance have drawn criticism from the corporate sector. The company faces mounting pressure to deliver more competitive AI solutions.
13. Apple’s MLX library is excellent. - While Apple’s broader AI strategy falters, its Machine Learning Experience (MLX) library stands out. Businesses praise MLX for its robust features and seamless integration capabilities. This tool has become a valuable resource for developers looking to harness AI in Apple’s ecosystem.
14. The rise of inference-scaling “reasoning” models. - Inference-scaling models have gained prominence for their ability to tackle complex reasoning tasks. These advanced models are transforming decision-making processes across industries. They represent a significant step forward in applying AI to real-world challenges.
15. The environmental impact got better. - Efforts to reduce the carbon footprint of AI have made measurable progress. Innovations in energy-efficient training methods and hardware have mitigated some environmental concerns. These steps align with corporate sustainability goals, fostering greater confidence in AI technologies.
16. The environmental impact got much, much worse. - However, the overall environmental cost of AI continues to rise due to the sheer scale of new deployments. The demand for larger models and training datasets exacerbates the carbon footprint. Balancing AI growth with sustainability remains a critical challenge.
17. The year of slop. - A surge in AI-generated content flooded the digital landscape with low-quality outputs. This trend underscores the need for better content moderation and refinement tools. Companies must prioritize quality to maintain credibility and user trust.
18. Synthetic training data works great. - Synthetic datasets have proven highly effective in training LLMs, offering a scalable alternative to real-world data. This approach reduces privacy concerns and enhances model generalization. Businesses can leverage synthetic data to build robust and ethical AI systems.
19. Knowledge is incredibly unevenly distributed. - The expertise required to develop and deploy LLMs remains concentrated in specific regions and organizations. This disparity limits the global potential of AI, particularly for under-resourced businesses. Addressing this knowledge gap is essential for equitable AI adoption.
20. Adoption of LLMs needs to be more evenly distributed. - While LLM adoption has surged, it remains uneven across industries and geographies and here at SCE. We have deployed AI tools for everyone in the company to use (https://copilot.microsoft.com) and our metrics indicate many have not taken advantage of these investments. However, there have been others who have adopted the use of AI tools in their day to day activities and are delivering value to the company as the quality of their work and the speed delivering it increases. Bridging this divide is key to unlocking SCE’s full potential.
21. LLMs need better criticism. - Constructive critique of LLMs has increased, highlighting their limitations alongside their strengths. Honest assessments are required to drive improvements in model reliability and ethical standards. This trend fosters a more balanced and informed approach to AI development.